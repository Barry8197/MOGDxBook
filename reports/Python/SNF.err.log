Traceback (most recent call last):
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/barryryan/anaconda3/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/barryryan/anaconda3/lib/python3.11/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# Load networks and perform SNF

# Load networks
all_graphs = {}
all_idx = []

for i , mod in enumerate(modalities) : 
    with open(f'./../../data/raw/{mod}_processed.pkl', 'rb') as file:
        loaded_data = pickle.load(file)

    if i == 0 : 
        datMeta = loaded_data['datMeta'].reset_index()
    else : 
        datMeta = pd.merge(datMeta , loaded_data['datMeta'].reset_index() , how = 'outer'  , on = [index_col , target])

    all_graphs[mod] =  nx.read_graphml(f'./../../data/Networks/{mod}_graph.graphml')
    all_idx.extend(list(all_graphs[mod].nodes))

# Merge metadata and create list of ids        
datMeta = datMeta.set_index(index_col)
all_idx = list(set(all_idx))

full_graphs = []

# Create network for each modality including patients wtihout data
for mod , graph in all_graphs.items() : 
    full_graph = pd.DataFrame(data = np.zeros((len(all_idx) , len(all_idx))) , index=all_idx , columns=all_idx)
    graph = nx.to_pandas_adjacency(graph)
    full_graph.loc[graph.index , graph.index] = graph.values
    
    full_graphs.append(full_graph)

#  Perform SNF
k = 15
adj = snf.snf(full_graphs , K = k , t = 10)
np.fill_diagonal(adj , 1)

adj_snf = pd.DataFrame(data=adj , index=all_idx , columns=all_idx)

node_labels = pd.Series(adj_snf.index) 

node_colour = datMeta.loc[adj_snf.index][target].astype('category').cat.set_categories(wesanderson.FantasticFox2_5.hex_colors , rename=True)

G  = preprocess_functions.plot_knn_network(adj_snf , k , datMeta.loc[adj_snf.index][target] ,
                                                       node_colours=node_colour , node_size=150)
snf_name = '_'.join(modalities)

nx.write_graphml(G, f'./../../data/Networks/{snf_name}_graph.graphml')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Cell [0;32mIn[3], line 9[0m
[1;32m      7[0m [38;5;28;01mfor[39;00m i , mod [38;5;129;01min[39;00m [38;5;28menumerate[39m(modalities) : 
[1;32m      8[0m     [38;5;28;01mwith[39;00m [38;5;28mopen[39m([38;5;124mf[39m[38;5;124m'[39m[38;5;124m./../../data/raw/[39m[38;5;132;01m{[39;00mmod[38;5;132;01m}[39;00m[38;5;124m_processed.pkl[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mrb[39m[38;5;124m'[39m) [38;5;28;01mas[39;00m file:
[0;32m----> 9[0m         loaded_data [38;5;241m=[39m pickle[38;5;241m.[39mload(file)
[1;32m     11[0m     [38;5;28;01mif[39;00m i [38;5;241m==[39m [38;5;241m0[39m : 
[1;32m     12[0m         datMeta [38;5;241m=[39m loaded_data[[38;5;124m'[39m[38;5;124mdatMeta[39m[38;5;124m'[39m][38;5;241m.[39mreset_index()

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/storage.py:337[0m, in [0;36m_load_from_bytes[0;34m(b)[0m
[1;32m    336[0m [38;5;28;01mdef[39;00m [38;5;21m_load_from_bytes[39m(b):
[0;32m--> 337[0m     [38;5;28;01mreturn[39;00m torch[38;5;241m.[39mload(io[38;5;241m.[39mBytesIO(b))

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:1028[0m, in [0;36mload[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)[0m
[1;32m   1026[0m     [38;5;28;01mexcept[39;00m [38;5;167;01mRuntimeError[39;00m [38;5;28;01mas[39;00m e:
[1;32m   1027[0m         [38;5;28;01mraise[39;00m pickle[38;5;241m.[39mUnpicklingError(UNSAFE_MESSAGE [38;5;241m+[39m [38;5;28mstr[39m(e)) [38;5;28;01mfrom[39;00m [38;5;28;01mNone[39;00m
[0;32m-> 1028[0m [38;5;28;01mreturn[39;00m _legacy_load(opened_file, map_location, pickle_module, [38;5;241m*[39m[38;5;241m*[39mpickle_load_args)

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:1256[0m, in [0;36m_legacy_load[0;34m(f, map_location, pickle_module, **pickle_load_args)[0m
[1;32m   1254[0m unpickler [38;5;241m=[39m UnpicklerWrapper(f, [38;5;241m*[39m[38;5;241m*[39mpickle_load_args)
[1;32m   1255[0m unpickler[38;5;241m.[39mpersistent_load [38;5;241m=[39m persistent_load
[0;32m-> 1256[0m result [38;5;241m=[39m unpickler[38;5;241m.[39mload()
[1;32m   1258[0m deserialized_storage_keys [38;5;241m=[39m pickle_module[38;5;241m.[39mload(f, [38;5;241m*[39m[38;5;241m*[39mpickle_load_args)
[1;32m   1260[0m offset [38;5;241m=[39m f[38;5;241m.[39mtell() [38;5;28;01mif[39;00m f_should_read_directly [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:1193[0m, in [0;36m_legacy_load.<locals>.persistent_load[0;34m(saved_id)[0m
[1;32m   1189[0m     obj[38;5;241m.[39m_torch_load_uninitialized [38;5;241m=[39m [38;5;28;01mTrue[39;00m
[1;32m   1190[0m     [38;5;66;03m# TODO: Once we decide to break serialization FC, we can[39;00m
[1;32m   1191[0m     [38;5;66;03m# stop wrapping with TypedStorage[39;00m
[1;32m   1192[0m     typed_storage [38;5;241m=[39m torch[38;5;241m.[39mstorage[38;5;241m.[39mTypedStorage(
[0;32m-> 1193[0m         wrap_storage[38;5;241m=[39mrestore_location(obj, location),
[1;32m   1194[0m         dtype[38;5;241m=[39mdtype,
[1;32m   1195[0m         _internal[38;5;241m=[39m[38;5;28;01mTrue[39;00m)
[1;32m   1196[0m     deserialized_objects[root_key] [38;5;241m=[39m typed_storage
[1;32m   1197[0m [38;5;28;01melse[39;00m:

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:381[0m, in [0;36mdefault_restore_location[0;34m(storage, location)[0m
[1;32m    379[0m [38;5;28;01mdef[39;00m [38;5;21mdefault_restore_location[39m(storage, location):
[1;32m    380[0m     [38;5;28;01mfor[39;00m _, _, fn [38;5;129;01min[39;00m _package_registry:
[0;32m--> 381[0m         result [38;5;241m=[39m fn(storage, location)
[1;32m    382[0m         [38;5;28;01mif[39;00m result [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
[1;32m    383[0m             [38;5;28;01mreturn[39;00m result

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:274[0m, in [0;36m_cuda_deserialize[0;34m(obj, location)[0m
[1;32m    272[0m [38;5;28;01mdef[39;00m [38;5;21m_cuda_deserialize[39m(obj, location):
[1;32m    273[0m     [38;5;28;01mif[39;00m location[38;5;241m.[39mstartswith([38;5;124m'[39m[38;5;124mcuda[39m[38;5;124m'[39m):
[0;32m--> 274[0m         device [38;5;241m=[39m validate_cuda_device(location)
[1;32m    275[0m         [38;5;28;01mif[39;00m [38;5;28mgetattr[39m(obj, [38;5;124m"[39m[38;5;124m_torch_load_uninitialized[39m[38;5;124m"[39m, [38;5;28;01mFalse[39;00m):
[1;32m    276[0m             [38;5;28;01mwith[39;00m torch[38;5;241m.[39mcuda[38;5;241m.[39mdevice(device):

File [0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:258[0m, in [0;36mvalidate_cuda_device[0;34m(location)[0m
[1;32m    255[0m device [38;5;241m=[39m torch[38;5;241m.[39mcuda[38;5;241m.[39m_utils[38;5;241m.[39m_get_device_index(location, [38;5;28;01mTrue[39;00m)
[1;32m    257[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m torch[38;5;241m.[39mcuda[38;5;241m.[39mis_available():
[0;32m--> 258[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m([38;5;124m'[39m[38;5;124mAttempting to deserialize object on a CUDA [39m[38;5;124m'[39m
[1;32m    259[0m                        [38;5;124m'[39m[38;5;124mdevice but torch.cuda.is_available() is False. [39m[38;5;124m'[39m
[1;32m    260[0m                        [38;5;124m'[39m[38;5;124mIf you are running on a CPU-only machine, [39m[38;5;124m'[39m
[1;32m    261[0m                        [38;5;124m'[39m[38;5;124mplease use torch.load with map_location=torch.device([39m[38;5;130;01m\'[39;00m[38;5;124mcpu[39m[38;5;130;01m\'[39;00m[38;5;124m) [39m[38;5;124m'[39m
[1;32m    262[0m                        [38;5;124m'[39m[38;5;124mto map your storages to the CPU.[39m[38;5;124m'[39m)
[1;32m    263[0m device_count [38;5;241m=[39m torch[38;5;241m.[39mcuda[38;5;241m.[39mdevice_count()
[1;32m    264[0m [38;5;28;01mif[39;00m device [38;5;241m>[39m[38;5;241m=[39m device_count:

[0;31mRuntimeError[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.

